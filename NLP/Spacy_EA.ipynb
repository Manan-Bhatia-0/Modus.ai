{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy nlp exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human\n",
      "ambition\n",
      "is\n",
      "the\n",
      "key\n",
      "to\n",
      "staying\n",
      "ahead\n",
      "of\n",
      "automation\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Human ambition is the key to staying ahead of automation.')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John\t0\tJohn\tPROPN\tNNP\tnsubj\tXxxx\tTrue\tFalse\n",
      "bought\t5\tbuy\tVERB\tVBD\tROOT\txxxx\tTrue\tFalse\n",
      "a\t12\ta\tDET\tDT\tdet\tx\tTrue\tTrue\n",
      "car\t14\tcar\tNOUN\tNN\tdobj\txxx\tTrue\tFalse\n",
      "and\t18\tand\tCCONJ\tCC\tcc\txxx\tTrue\tTrue\n",
      "Mary\t22\tMary\tPROPN\tNNP\tconj\tXxxx\tTrue\tFalse\n",
      "a\t27\ta\tDET\tDT\tdet\tx\tTrue\tTrue\n",
      "motorcycle\t29\tmotorcycle\tNOUN\tNN\tappos\txxxx\tTrue\tFalse\n",
      ".\t39\t.\tPUNCT\t.\tpunct\t.\tFalse\tFalse\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'John bought a car and Mary a motorcycle.')\n",
    "\n",
    "for token in doc:\n",
    "    print(\"\\t\".join( (token.text, str(token.idx), token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, str(token.is_alpha), str(token.is_stop) )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John nsubj bought VERB []\n",
      "bought ROOT bought VERB [John, car, .]\n",
      "a det car NOUN []\n",
      "car dobj bought VERB [a, and, Mary]\n",
      "and cc car NOUN []\n",
      "Mary conj car NOUN [motorcycle]\n",
      "a det motorcycle NOUN []\n",
      "motorcycle appos Mary PROPN [a]\n",
      ". punct bought VERB []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John 0 4 PERSON\n",
      "Mary 22 26 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Ali Hassan Kuban said that Apple Inc. will buy Google in May 2018.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ali Hassan Kuban 0 16 PERSON\n",
      "Apple Inc. 27 37 ORG\n",
      "Google 47 53 ORG\n",
      "May 2018 57 65 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"John met Peter and Susan called Paul.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " decided to fire \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tim Cook\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and hire somebody called \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Doe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " as the new CEO.</br>They also discussed a merger with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". On the long run it seems more likely that \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</br>will merge with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". The companies will all relocate to</br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Austin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Texas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " before \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the end of the century\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John Doe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " bought a \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Porsche\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"Apple decided to fire Tim Cook and hire somebody called John Doe as the new CEO.\n",
    "They also discussed a merger with Google. On the long run it seems more likely that Apple\n",
    "will merge with Amazon and Microsoft with Google. The companies will all relocate to\n",
    "Austin in Texas before the end of the century. John Doe bought a Porsche.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'apples and dogs are bananas and cats person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples apples 1.0\n",
      "apples and 0.044306833\n",
      "apples dogs 0.4178702\n",
      "apples are 0.019538091\n",
      "apples bananas 0.6533332\n",
      "apples and 0.017371425\n",
      "apples cats 0.27148277\n",
      "apples person 0.13093166\n",
      "and apples 0.044306833\n",
      "and and 1.0\n",
      "and dogs 0.043407787\n",
      "and are 0.2009429\n",
      "and bananas 0.11184211\n",
      "and and 1.0\n",
      "and cats 0.015565561\n",
      "and person -0.11508114\n",
      "dogs apples 0.4178702\n",
      "dogs and 0.043407787\n",
      "dogs dogs 1.0\n",
      "dogs are 0.013911209\n",
      "dogs bananas 0.44729885\n",
      "dogs and 0.013348826\n",
      "dogs cats 0.52501935\n",
      "dogs person 0.15286715\n",
      "are apples 0.019538091\n",
      "are and 0.2009429\n",
      "are dogs 0.013911209\n",
      "are are 1.0\n",
      "are bananas 0.2086141\n",
      "are and 0.14163685\n",
      "are cats -0.01583488\n",
      "are person 0.1131013\n",
      "bananas apples 0.6533332\n",
      "bananas and 0.11184211\n",
      "bananas dogs 0.44729885\n",
      "bananas are 0.2086141\n",
      "bananas bananas 1.0\n",
      "bananas and 0.17506078\n",
      "bananas cats 0.31777847\n",
      "bananas person 0.09854238\n",
      "and apples 0.017371425\n",
      "and and 1.0\n",
      "and dogs 0.013348826\n",
      "and are 0.14163685\n",
      "and bananas 0.17506078\n",
      "and and 1.0\n",
      "and cats 0.09988121\n",
      "and person -0.10388825\n",
      "cats apples 0.27148277\n",
      "cats and 0.015565561\n",
      "cats dogs 0.52501935\n",
      "cats are -0.01583488\n",
      "cats bananas 0.31777847\n",
      "cats and 0.09988121\n",
      "cats cats 1.0\n",
      "cats person 0.089055665\n",
      "person apples 0.13093166\n",
      "person and -0.11508114\n",
      "person dogs 0.15286715\n",
      "person are 0.1131013\n",
      "person bananas 0.09854238\n",
      "person and -0.10388825\n",
      "person cats 0.089055665\n",
      "person person 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashmiananth/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1, token2, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.1705783234813302\n",
      "2: 0.2751764529693607\n",
      "3: 0.1957795219474742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashmiananth/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"The labrador barked.\")\n",
    "doc2 = nlp(u\"The labrador swam.\")\n",
    "doc3 = nlp(u\"the labrador people live in canada.\")\n",
    "\n",
    "dog = nlp(u\"dog\")\n",
    "\n",
    "count = 0\n",
    "for doc in [doc1, doc2, doc3]:\n",
    "    lab = doc[1]\n",
    "    count += 1\n",
    "    print(str(count) + \":\", lab.similarity(dog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1.0\n",
      "0 1 0.8045532009754652\n",
      "0 2 0.6841753801186126\n",
      "1 0 0.8045532009754652\n",
      "1 1 1.0\n",
      "1 2 0.5060953510737396\n",
      "2 0 0.6841753801186126\n",
      "2 1 0.5060953510737396\n",
      "2 2 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashmiananth/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "docs = ( nlp(u\"Paris is the largest city in France.\"),\n",
    "        nlp(u\"Vilnius is the capital of Lithuania.\"),\n",
    "        nlp(u\"An emu is a large bird.\") )\n",
    "\n",
    "for x in range(len(docs)):\n",
    "    for y in range(len(docs)):\n",
    "        print(x, y, docs[x].similarity(docs[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dog bites man\" \"dog bites man\" 1.0\n",
      "\"dog bites man\" \"man bites dog\" 0.8422244882281177\n",
      "\"dog bites man\" \"man dog bites\" 0.8688251513379205\n",
      "\"dog bites man\" \"cat eats mouse\" 0.33363362907991617\n",
      "\"man bites dog\" \"dog bites man\" 0.8422244882281177\n",
      "\"man bites dog\" \"man bites dog\" 1.0\n",
      "\"man bites dog\" \"man dog bites\" 0.8189262546757138\n",
      "\"man bites dog\" \"cat eats mouse\" 0.3792663613754763\n",
      "\"man dog bites\" \"dog bites man\" 0.8688251513379205\n",
      "\"man dog bites\" \"man bites dog\" 0.8189262546757138\n",
      "\"man dog bites\" \"man dog bites\" 1.0\n",
      "\"man dog bites\" \"cat eats mouse\" 0.25610488284650623\n",
      "\"cat eats mouse\" \"dog bites man\" 0.33363362907991617\n",
      "\"cat eats mouse\" \"man bites dog\" 0.3792663613754763\n",
      "\"cat eats mouse\" \"man dog bites\" 0.25610488284650623\n",
      "\"cat eats mouse\" \"cat eats mouse\" 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rashmiananth/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "docs = [nlp(u\"dog bites man\"), nlp(u\"man bites dog\"),\n",
    "        nlp(u\"man dog bites\"), nlp(u\"cat eats mouse\")]\n",
    "\n",
    "for doc in docs:\n",
    "    for other_doc in docs:\n",
    "        print('\"' + doc.text + '\"', '\"' + other_doc.text + '\"', doc.similarity(other_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/aspect-based-sentiment-analysis-using-spacy-textblob-4c8de3e0d2b9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'The food we had yesterday was delicious',\n",
    "  'My time in Italy was very enjoyable',\n",
    "  'I found the meal to be tasty',\n",
    "  'The internet was slow.',\n",
    "  'Our experience was suboptimal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det food NOUN DET []\n",
      "food nsubj was AUX NOUN [The, had]\n",
      "we nsubj had VERB PRON []\n",
      "had relcl food NOUN VERB [we, yesterday]\n",
      "yesterday npadvmod had VERB NOUN []\n",
      "was ROOT was AUX AUX [food, delicious]\n",
      "delicious acomp was AUX ADJ []\n",
      "My poss time NOUN PRON []\n",
      "time nsubj was AUX NOUN [My, in]\n",
      "in prep time NOUN ADP [Italy]\n",
      "Italy pobj in ADP PROPN []\n",
      "was ROOT was AUX AUX [time, enjoyable]\n",
      "very advmod enjoyable ADJ ADV []\n",
      "enjoyable acomp was AUX ADJ [very]\n",
      "I nsubj found VERB PRON []\n",
      "found ROOT found VERB VERB [I, be]\n",
      "the det meal NOUN DET []\n",
      "meal nsubj be VERB NOUN [the]\n",
      "to aux be VERB PART []\n",
      "be ccomp found VERB VERB [meal, to, tasty]\n",
      "tasty acomp be VERB ADJ []\n",
      "The det internet NOUN DET []\n",
      "internet nsubj was AUX NOUN [The]\n",
      "was ROOT was AUX AUX [internet, slow, .]\n",
      "slow acomp was AUX ADJ []\n",
      ". punct was AUX PUNCT []\n",
      "Our poss experience NOUN PRON []\n",
      "experience nsubj was AUX NOUN [Our]\n",
      "was ROOT was AUX AUX [experience, suboptimal]\n",
      "suboptimal acomp was AUX ADJ []\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            token.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = ''\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            descriptive_term = token\n",
    "    print(sentence)\n",
    "    print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The food we had yesterday was delicious\n",
      "delicious\n",
      "My time in Italy was very enjoyable\n",
      "very enjoyable\n",
      "I found the meal to be tasty\n",
      "tasty\n",
      "The internet was slow.\n",
      "slow\n",
      "Our experience was suboptimal\n",
      "suboptimal\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': 'delicious'}, {'aspect': 'time', 'description': 'very enjoyable'}, {'aspect': 'meal', 'description': 'tasty'}, {'aspect': 'internet', 'description': 'slow'}, {'aspect': 'experience', 'description': 'suboptimal'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  target = ''\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "      target = token.text\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  aspects.append({'aspect': target,\n",
    "    'description': descriptive_term})\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'aspect': 'food',\n",
       "  'description': 'delicious',\n",
       "  'sentiment': Sentiment(polarity=1.0, subjectivity=1.0)},\n",
       " {'aspect': 'time',\n",
       "  'description': 'very enjoyable',\n",
       "  'sentiment': Sentiment(polarity=0.65, subjectivity=0.78)},\n",
       " {'aspect': 'meal',\n",
       "  'description': 'tasty',\n",
       "  'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)},\n",
       " {'aspect': 'internet',\n",
       "  'description': 'slow',\n",
       "  'sentiment': Sentiment(polarity=-0.30000000000000004, subjectivity=0.39999999999999997)},\n",
       " {'aspect': 'experience',\n",
       "  'description': 'suboptimal',\n",
       "  'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "  aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious food.\n",
      "positive\n",
      "\n",
      "\n",
      "Very Slow internet.\n",
      "negative\n",
      "\n",
      "\n",
      "Suboptimal experience.\n",
      "negative\n",
      "\n",
      "\n",
      "Enjoyable food.\n",
      "positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "# We train the NaivesBayesClassifier\n",
    "train = [\n",
    "  ('Slow internet.', 'negative'),\n",
    "  ('Delicious food', 'positive'),\n",
    "  ('Suboptimal experience', 'negative'),\n",
    "  ('Very enjoyable time', 'positive'),\n",
    "  ('delicious food.', 'neg')\n",
    "]\n",
    "cl = NaiveBayesClassifier(train)\n",
    "# And then we try to classify some sample sentences.\n",
    "blob = TextBlob(\"Delicious food. Very Slow internet. Suboptimal experience. Enjoyable food.\", classifier=cl)\n",
    "for s in blob.sentences:\n",
    "    print(s)\n",
    "    print(s.classify())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
